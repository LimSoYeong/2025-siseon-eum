import sys
import time
import json
import torch
from pathlib import Path
from PIL import Image

from model_loader import get_model, get_processor

# ====== ê²½ë¡œ ì„¤ì • ======
base_dir = Path(__file__).resolve().parent.parent
result_path = base_dir / "qwen" / "results_time_infer_only.jsonl"
image_dir = base_dir / "data" / "img"
image_files = sorted(image_dir.glob("*.jpg"))  # ë˜ëŠ” *.png ë“±

# ====== ì €ì¥ í•¨ìˆ˜ ======
def save_result_jsonl(output, infer_time, path=result_path):
    record = {
        "output": output.strip(),
        "infer_time": round(infer_time, 2)
    }
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

# ====== ë¶„ë¥˜ìš© í”„ë¡¬í”„íŠ¸ ======
DOC_TYPE_PROMPT = """
ë‹¤ìŒ ë¬¸ì„œê°€ ì–´ë–¤ ìœ í˜•ì¸ì§€ í•˜ë‚˜ë§Œ ì„ íƒí•´ì„œ ì¶œë ¥í•˜ì„¸ìš”.
- ê³ ì§€ì„œ
- ì•ˆë‚´ë¬¸-ê±´ê°•
- ì•ˆë‚´ë¬¸-ìƒí™œ
- ì•ˆë‚´ë¬¸-ê¸ˆìœµ

ì¡°ê±´:
- ìœ í˜• ì´ë¦„ë§Œ ì •í™•íˆ ì¶œë ¥í•˜ì„¸ìš” (ì˜ˆ: ê³ ì§€ì„œ, ì•ˆë‚´ë¬¸-ìƒí™œ)
- ë‹¤ë¥¸ ì„¤ëª…ì€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”
"""

# ====== ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸ (ìœ í˜•ë³„ë¡œ ë³€ìˆ˜ ë”°ë¡œ) ======
A_PROMPT = "ê³ ì§€ì„œì•¼. ì°¸ê³ í•´ì„œ ë…¸ì¸ë¶„ë“¤ ëŒ€ìƒìœ¼ë¡œ ìš”ì•½í•´ì¤˜"
HEALTH_PROMPT = "ê±´ê°•ê´€ë ¨ ì•ˆë‚´ë¬¸ì´ì•¼. ì°¸ê³ í•´ì„œ ë…¸ì¸ë¶„ë“¤ ëŒ€ìƒìœ¼ë¡œ ìš”ì•½í•´ì¤˜"
LIFE_PROMPT = "ìƒí™œê´€ë ¨ ì•ˆë‚´ë¬¸ì´ì•¼. ì°¸ê³ í•´ì„œ ë…¸ì¸ë¶„ë“¤ ëŒ€ìƒìœ¼ë¡œ ìš”ì•½í•´ì¤˜"
FINANCE_PROMPT = "ê¸ˆìœµê´€ë ¨ ì•ˆë‚´ë¬¸ì´ì•¼. ì°¸ê³ í•´ì„œ ë…¸ì¸ë¶„ë“¤ ëŒ€ìƒìœ¼ë¡œ ìš”ì•½í•´ì¤˜"

# ====== ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ======
model = get_model().eval()
processor = get_processor()
print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {model.device})")

# ====== ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜ í•¨ìˆ˜ ======
def classify_document(image, model, processor):
    messages = [
        {"role": "user", "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": DOC_TYPE_PROMPT}
        ]}
    ]
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(text=[text], images=[image], return_tensors="pt").to(model.device)

    with torch.no_grad():
        generated_ids = model.generate(**inputs, max_new_tokens=16)
        trimmed_ids = [out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)]
        result = processor.batch_decode(trimmed_ids, skip_special_tokens=True)[0]
    return result.strip()

# ====== ë©”ì¸ ë£¨í”„ ======
for img_path in image_files:
    image_id = img_path.stem
    try:
        image = Image.open(img_path).convert("RGB")
    except Exception as e:
        print(f"âš ï¸ ì´ë¯¸ì§€ ì—´ê¸° ì‹¤íŒ¨: {img_path} - {e}")
        continue

    print(f"\nğŸ–¼ï¸ [{image_id}] ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘...")

    # 1. ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜
    doc_type = classify_document(image, model, processor)
    print(f"ğŸ” ë¬¸ì„œ ìœ í˜•: {doc_type}")

    # 2. í”„ë¡¬í”„íŠ¸ ì„ íƒ (ë³€ìˆ˜ë¡œ ì§ì ‘ í• ë‹¹)
    if doc_type == "ê³ ì§€ì„œ":
        prompt_text = A_PROMPT
    elif doc_type == "ì•ˆë‚´ë¬¸-ê±´ê°•":
        prompt_text = HEALTH_PROMPT
    elif doc_type == "ì•ˆë‚´ë¬¸-ìƒí™œ":
        prompt_text = LIFE_PROMPT
    elif doc_type == "ì•ˆë‚´ë¬¸-ê¸ˆìœµ":
        prompt_text = FINANCE_PROMPT
    else:
        print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ìœ í˜•: {doc_type}")
        continue

    # 3. ìš”ì•½ìš© ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        {"role": "user", "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": prompt_text}
        ]}
    ]
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(text=[text], images=[image], return_tensors="pt").to(model.device)

    # 4. ì¶”ë¡ 
    with torch.no_grad():
        start = time.time()
        generated_ids = model.generate(**inputs, max_new_tokens=512)
        end = time.time()

        trimmed_ids = [out[len(inp):] for inp, out in zip(inputs.input_ids, generated_ids)]
        output = processor.batch_decode(trimmed_ids, skip_special_tokens=True)[0]

    infer_time = end - start
    print(f"ğŸ“ [{image_id}] ìš”ì•½ ì™„ë£Œ ({infer_time:.2f}s)\nâ†’ {output.strip()}")
    save_result_jsonl(output, infer_time)
