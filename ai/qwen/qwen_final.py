import time, json, gc
from pathlib import Path
from PIL import Image
import torch

from model_loader import get_model, get_processor

# ===== ê²½ë¡œ ì„¤ì • =====
base_dir = Path(__file__).resolve().parent.parent
out_path = base_dir / "qwen" / "results_hybrid_seq.jsonl"
image_dir = base_dir / "data" / "img"
image_files = sorted(list(image_dir.glob("*.jpg")) + list(image_dir.glob("*.png")))

# ====== ë¶„ë¥˜ìš© í”„ë¡¬í”„íŠ¸ ======
DOC_TYPE_PROMPT = """
ë‹¤ìŒ ë¬¸ì„œê°€ ì–´ë–¤ ìœ í˜•ì¸ì§€ í•˜ë‚˜ë§Œ ì„ íƒí•´ì„œ ì¶œë ¥í•˜ì„¸ìš”.
- ê³ ì§€ì„œ
- ì•ˆë‚´ë¬¸-ê±´ê°•
- ì•ˆë‚´ë¬¸-ìƒí™œ
- ì•ˆë‚´ë¬¸-ê¸ˆìœµ
- ê¸°íƒ€

ì¡°ê±´:
- ìœ í˜• ì´ë¦„ë§Œ ì •í™•íˆ ì¶œë ¥í•˜ì„¸ìš” (ì˜ˆ: ê³ ì§€ì„œ, ì•ˆë‚´ë¬¸-ìƒí™œ, ê¸°íƒ€)
- ë‹¤ë¥¸ ì„¤ëª…ì€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”
"""

# ====== ìš”ì•½ìš© í”„ë¡¬í”„íŠ¸ (ìœ í˜•ë³„ë¡œ ë³€ìˆ˜ ë”°ë¡œ) ======
A_PROMPT = """
ë‹¤ìŒ ë¬¸ì„œë¥¼ ë³´ê³  ì–´ë¥´ì‹ ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.

ë‹¨ê³„:
    1. ë¬¸ì„œë¥¼ ì½ê³  1ì°¨ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.
    2. ì•„ë˜ ì§ˆë¬¸ì„ ìŠ¤ìŠ¤ë¡œì—ê²Œ í•˜ë©° ê²€í† í•˜ê³ , ìˆ˜ì •ì´ í•„ìš”í•˜ë‹¤ë©´ ë³´ì™„í•˜ì—¬ ìµœì¢… ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.

ê²€í†  ì§ˆë¬¸:
    - ë¹ ëœ¨ë¦° ì¤‘ìš”í•œ í•­ëª©(ë‚©ë¶€ ì´ ê¸ˆì•¡, ë‚ ì§œ, ë‚©ë¶€ ë§ˆê°ì¼ ë“±)ì€ ì—†ë‚˜ìš”?
    - ì´ ê³ ì§€ì„œê°€ ë¬´ìŠ¨ ëˆ(ì„¸ê¸ˆÂ·ìš”ê¸ˆ) ê³ ì§€ì„œì¸ì§€ ëª…í™•í•˜ê²Œ ì•Œë ¤ì£¼ì—ˆë‚˜ìš”? (ì˜ˆ: ìë™ì°¨ì„¸, ì „ê¸°ìš”ê¸ˆ, ê±´ê°•ë³´í—˜ë£Œ ë“±)
    - ì–´ë¥´ì‹ ì´ ë“£ê¸°ì— ì´í•´í•˜ê¸° ì–´ë ¤ìš´ í‘œí˜„ì€ ì—†ë‚˜ìš”?
    - ê¸ˆì•¡ê³¼ ë§ˆê°ì¼ì€ ì¹œìˆ™í•œ ë‹¨ìœ„ì™€ ë§íˆ¬ë¡œ í‘œí˜„ë˜ì—ˆë‚˜ìš”?

ì¶œë ¥ ì¡°ê±´:
    - ìµœì¢… 2~3ë¬¸ì¥ ìš”ì•½ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
    - ê²€í†  ê³¼ì •ì´ë‚˜ ì¤‘ê°„ ìš”ì•½ì€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.

"""

    
HEALTH_PROMPT = """
    ë‹¤ìŒ ê±´ê°• ì•ˆë‚´ë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë¥´ì‹ ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.  
    ì˜ˆì‹œ, ìê°€ ì ê²€ ì§ˆë¬¸, í•µì‹¬ ìš”ì†Œë¥¼ ì°¸ê³ í•˜ë˜,  
    ì¤‘ê°„ ê³¼ì •ì€ ì¶œë ¥í•˜ì§€ ë§ê³ , 'ì‰¬ìš´ ë§ ìš”ì•½:' í•œ ì¤„ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

    ì˜ˆì‹œ:
        ì›ë¬¸:
            "â—‹ 8ì›” 10ì¼ ì˜¤ì „ 9ì‹œë¶€í„° ë¬´ë£Œ ê±´ê°•ê²€ì§„ì´ ì§„í–‰ë©ë‹ˆë‹¤.
            â—‹ ì¥ì†Œ: â—‹â—‹ë³´ê±´ì†Œ
            â—‹ ì¤€ë¹„ë¬¼: ì‹ ë¶„ì¦ ì§€ì°¸"
        ì‰¬ìš´ ë§ ìš”ì•½:
            "ì–´ë¥´ì‹ , 8ì›” 10ì¼ ì˜¤ì „ 9ì‹œì— â—‹â—‹ë³´ê±´ì†Œì—ì„œ ê±´ê°•ê²€ì§„ì´ ìˆì–´ìš”. ì‹ ë¶„ì¦ ê¼­ ì±™ê¸°ì„¸ìš”."

    ì§ˆë¬¸:
        1. ë‚ ì§œì™€ ì‹œê°„ì´ ëª…í™•í•œê°€ìš”?
        2. ì¥ì†Œì™€ ì¤€ë¹„ë¬¼ì´ ë¹ ì§€ì§€ ì•Šì•˜ë‚˜ìš”?
        3. ì–´ë¥´ì‹ ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ë§íˆ¬ë¡œ í‘œí˜„ë˜ì—ˆë‚˜ìš”?

    í•„ìˆ˜ ìš”ì†Œ:
        - ë‚ ì§œì™€ ì‹œê°„
        - ì¥ì†Œ
        - ëŒ€ìƒì ë˜ëŠ” ì‹ ì²­ ì¡°ê±´
        - ì¤€ë¹„ë¬¼ (ì˜ˆ: ì‹ ë¶„ì¦)
        - ì£¼ì˜ì‚¬í•­ (ìˆëŠ” ê²½ìš°)

    ë‹¨ê³„:
        1ë‹¨ê³„: ìœ„ í•­ëª©ì„ í‘œì²˜ëŸ¼ ì •ë¦¬í•˜ê³  ìš”ì•½ í›„ë³´ë¥¼ ì‘ì„±í•˜ì„¸ìš”.  
        2ë‹¨ê³„: ë¹ ì§„ ì •ë³´ ë³´ì™„ í›„ 5ë¬¸ì¥ ìš”ì•½ìœ¼ë¡œ ì••ì¶•í•˜ì„¸ìš”.  
        3ë‹¨ê³„: ë¬¸ì¥ ìˆ˜ë¥¼ ì¤„ì´ë©° ë¶ˆí•„ìš”í•œ í‘œí˜„ì€ ì œê±°í•˜ì„¸ìš”.  
        4ë‹¨ê³„: í•µì‹¬ë§Œ ë‚¨ê²¨ 2~3ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.  
        5ë‹¨ê³„: ë§íˆ¬ë¥¼ ì–´ë¥´ì‹ ê»˜ ì¹œê·¼í•˜ê²Œ ë‹¤ë“¬ìœ¼ì„¸ìš”.

    ì¶œë ¥ ì¡°ê±´:
        - ë°˜ë“œì‹œ 'ì‰¬ìš´ ë§ ìš”ì•½:'ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” 2~3ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        - 1~4 ë‹¨ê³„ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.

    """

LIFE_PROMPT = """
    ë‹¤ìŒ ìƒí™œ ì•ˆë‚´ë¬¸ì„ ìš”ì•½í•  ë•Œ, ì•„ë˜ì˜ í•„ìˆ˜ ì •ë³´ê°€ í¬í•¨ë˜ë„ë¡  
    ìŠ¤ìŠ¤ë¡œ ì ê²€í•˜ë©° ë¬¸ì¥ì„ ì •ë¦¬í•˜ê³ , ìµœì¢… ìš”ì•½ì€ 2~3ë¬¸ì¥ìœ¼ë¡œ ì••ì¶•í•˜ì„¸ìš”.

    ë°˜ë“œì‹œ í¬í•¨í•  ì •ë³´:
        - í–‰ì‚¬ëª… ë˜ëŠ” ëª©ì 
        - ë‚ ì§œì™€ ì‹œê°„
        - ì¥ì†Œ
        - ì‹ ì²­ ë°©ë²• ë˜ëŠ” ì—°ë½ì²˜
        - ìœ ì˜ì‚¬í•­ ë˜ëŠ” ì¤€ë¹„ë¬¼

    ìŠ¤ìŠ¤ë¡œ ì ê²€í•˜ì„¸ìš”:
        - ì¼ì •, ì¥ì†Œ, ì‹ ì²­ ë°©ë²•ì´ ëª…í™•í•˜ê²Œ ì „ë‹¬ë˜ì—ˆë‚˜ìš”?
        - ë„ˆë¬´ ë”±ë”±í•˜ê±°ë‚˜ ì–´ìƒ‰í•œ í‘œí˜„ì€ ì—†ë‚˜ìš”?
        - ì–´ë¥´ì‹ ì´ í˜¼ë™ ì—†ì´ ì´í•´í•  ìˆ˜ ìˆì„ê¹Œìš”?

    ë‹¨ê³„:
        1. ìœ„ ì •ë³´ë¥¼ í¬í•¨í•œ ìƒì„¸í•œ ë¬¸ì¥ì„ ë– ì˜¬ë ¤ë³´ì„¸ìš”.
        2. ì¤‘ë³µë˜ê±°ë‚˜ ë¶ˆí•„ìš”í•œ í‘œí˜„ì„ ì •ë¦¬í•˜ê³  ë¬¸ì¥ì„ ì••ì¶•í•˜ì„¸ìš”.
        3. 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ì„ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.

    ì¶œë ¥ ì¡°ê±´:
        - ìµœì¢… ìš”ì•½ 2~3ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        - 1~2 ë‹¨ê³„ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
    """

FINANCE_PROMPT = """
    ë‹¤ìŒ ê¸ˆìœµ ì•ˆë‚´ë¬¸ì„ ë³´ê³  ì–´ë¥´ì‹ ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ìš”ì•½ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

    ë‹¨ê³„:
        1. í˜œíƒ, ê¸ˆì•¡, ì¡°ê±´, ì‹ ì²­ë°©ë²•, ê¸°í•œ ë“± ì¤‘ìš”í•œ ì •ë³´ë¥¼ í•­ëª©ë³„ë¡œ ë‚˜ì—´í•˜ì„¸ìš”.
        2. í•­ëª©ë“¤ì„ ê°„ë‹¨í•œ ë¬¸ì¥ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”.
        3. ë¹„ìŠ·í•œ ì •ë³´ëŠ” í•©ì¹˜ê³  ë¶ˆí•„ìš”í•œ ì„¤ëª…ì€ ì¤„ì´ì„¸ìš”.
        4. ì–´ë¥´ì‹ ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ë§ë¡œ 2~3ë¬¸ì¥ìœ¼ë¡œ ë§ˆë¬´ë¦¬ ìš”ì•½í•˜ì„¸ìš”.

    ì¶œë ¥ ì¡°ê±´:
        - ìµœì¢… ìš”ì•½ë§Œ ì¶œë ¥í•˜ì„¸ìš” (2~3ë¬¸ì¥)
        - ì¤‘ê°„ ë‹¨ê³„ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”
    """
ELSE_PROMPT = """
    ë…¸ì¸ë¶„ë“¤ ëŒ€ìƒìœ¼ë¡œ ì´ ë¬¸ì„œë¥¼ ìš”ì•½í•˜ì„¸ìš”.
    """


PROMPT_MAP = {
    "ê³ ì§€ì„œ": A_PROMPT,
    "ì•ˆë‚´ë¬¸-ê±´ê°•": HEALTH_PROMPT,
    "ì•ˆë‚´ë¬¸-ìƒí™œ": LIFE_PROMPT,
    "ì•ˆë‚´ë¬¸-ê¸ˆìœµ": FINANCE_PROMPT,
    "ê¸°íƒ€": ELSE_PROMPT,
}

MAX_NEW_TOKENS = 256
WARMUP = 0
# VRAM ì—¬ìœ ê°€ í¬ë©´ Trueë¡œ ë‘ ëª¨ë¸ ë™ì‹œ ë¡œë“œ(ë¹ ë¦„), ë¶€ì¡±í•˜ë©´ Falseë¡œ ë§¤ë²ˆ êµì²´ ë¡œë“œ(ì•ˆì •)
HOLD_BOTH_MODELS = False


# ===== ìœ í‹¸ =====
def save_jsonl(record):
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

def clear_model_cache():
    get_model.cache_clear()
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()

def classify_document(image, model, processor):
    messages = [
        {"role": "user", "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": DOC_TYPE_PROMPT}
        ]}
    ]
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(text=[text], images=[image], return_tensors="pt").to(model.device)
    with torch.no_grad():
        torch.cuda.synchronize(); t0 = time.perf_counter()
        out_ids = model.generate(**inputs, max_new_tokens=16)
        torch.cuda.synchronize(); t1 = time.perf_counter()
    trimmed = [o[len(i):] for i, o in zip(inputs.input_ids, out_ids)]
    result = processor.batch_decode(trimmed, skip_special_tokens=True)[0].strip()
    return result, (t1 - t0)

def summarize_document(image, prompt_text, model, processor, max_new_tokens=256):
    messages = [
        {"role": "user", "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": prompt_text}
        ]}
    ]
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = processor(text=[text], images=[image], return_tensors="pt").to(model.device)
    with torch.no_grad():
        torch.cuda.synchronize(); t0 = time.perf_counter()
        out_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)
        torch.cuda.synchronize(); t1 = time.perf_counter()
    trimmed = [o[len(i):] for i, o in zip(inputs.input_ids, out_ids)]
    output = processor.batch_decode(trimmed, skip_special_tokens=True)[0].strip()
    return output, (t1 - t0)

# ===== ë©”ì¸ =====
if __name__ == "__main__":
    if not image_files:
        raise SystemExit("ì´ë¯¸ì§€ í´ë”ê°€ ë¹„ì–´ìˆì–´ìš”: data/img/*.jpg|*.png")

    processor = get_processor()
    print(f"ì´ {len(image_files)}ì¥ ì²˜ë¦¬ ì‹œì‘ (ë¶„ë¥˜=Flash ON, ìš”ì•½=SDPA)")

    # ì„ íƒ 1) ë‘ ëª¨ë¸ ë™ì‹œ ë¡œë“œ
    model_cls = model_sum = None
    if HOLD_BOTH_MODELS:
        try:
            model_cls = get_model(attn_impl="flash_attention_2").eval()
        except Exception as e:
            print(f"âš ï¸ FlashAttention ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\nâ†’ sdpaë¡œ ëŒ€ì²´")
            model_cls = get_model(attn_impl="sdpa").eval()
        try:
            model_sum = get_model(attn_impl="sdpa").eval()
        except Exception as e:
            print(f"âŒ SDPA ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    # ì›Œë°ì—…(ì„ íƒ)
    if WARMUP > 0 and image_files:
        img0 = Image.open(image_files[0]).convert("RGB")
        if HOLD_BOTH_MODELS:
            _ = classify_document(img0, model_cls, processor)
            _ = summarize_document(img0, ELSE_PROMPT, model_sum, processor, MAX_NEW_TOKENS)
        else:
            # ë¶„ë¥˜ìš© ë¡œë“œ
            clear_model_cache()
            try:
                model = get_model(attn_impl="flash_attention_2").eval()
            except Exception:
                model = get_model(attn_impl="sdpa").eval()
            _ = classify_document(img0, model, processor)
            # ìš”ì•½ìš© ë¡œë“œ
            clear_model_cache()
            model = get_model(attn_impl="sdpa").eval()
            _ = summarize_document(img0, ELSE_PROMPT, model, processor, MAX_NEW_TOKENS)

    # ë³¸ ì²˜ë¦¬: ì´ë¯¸ì§€ë³„ë¡œ (ë¶„ë¥˜â†’ìš”ì•½) ìˆœì„œ ìˆ˜í–‰
    for img_path in image_files:
        try:
            image = Image.open(img_path).convert("RGB")
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ì—´ê¸° ì‹¤íŒ¨: {img_path} - {e}")
            continue

        # ----- ë¶„ë¥˜ (Flash ON ìš°ì„ ) -----
        if HOLD_BOTH_MODELS:
            m_cls = model_cls
        else:
            clear_model_cache()
            try:
                m_cls = get_model(attn_impl="flash_attention_2").eval()
            except Exception as e:
                print(f"âš ï¸ FlashAttention ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}\nâ†’ sdpaë¡œ ëŒ€ì²´")
                m_cls = get_model(attn_impl="sdpa").eval()

        doc_type, t_cls = classify_document(image, m_cls, processor)
        prompt_text = PROMPT_MAP.get(doc_type, ELSE_PROMPT)

        # ----- ìš”ì•½ (SDPA) -----
        if HOLD_BOTH_MODELS:
            m_sum = model_sum
        else:
            clear_model_cache()
            m_sum = get_model(attn_impl="sdpa").eval()

        output, t_sum = summarize_document(image, prompt_text, m_sum, processor, MAX_NEW_TOKENS)

        total_s = round(t_cls + t_sum, 4)
        rec = {
            "mode": "Hybrid-Sequential(FlashON-CLS,SDPA-SUM)",
            "image": img_path.name,
            "doc_type": doc_type,
            "output": output,
            "classify_s": round(t_cls, 4),
            "summary_s": round(t_sum, 4),
            "total_s": total_s,
            "attn_impl_cls": getattr(m_cls.config, "_attn_implementation", "unknown"),
            "attn_impl_sum": getattr(m_sum.config, "_attn_implementation", "unknown") if HOLD_BOTH_MODELS else "sdpa",
            "max_new_tokens": MAX_NEW_TOKENS,
        }
        save_jsonl(rec)
        print(f"ğŸ–¼ï¸ {img_path.name} | {doc_type} | cls {rec['classify_s']}s / sum {rec['summary_s']}s / total {total_s}s")
        print(f"â†’ ìš”ì•½: {output}\n")

    print(f"\nâœ… ì™„ë£Œ. ê²°ê³¼ ì €ì¥: {out_path}")