# backend/test/test_vlm_chat.py
# import sys
# import os
# # í˜„ì¬ backend í´ë”ë¥¼ PYTHONPATHì— ì¶”ê°€
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# from runnables.vlm_runnable import vlm_runnable

# if __name__ == "__main__":
#     image_path = "/root/2025-siseon-eum/ai/data/img/img_001.jpg"
#     with open(image_path, "rb") as f:
#         image_bytes = f.read()
#         output = vlm_runnable.invoke(image_bytes)
#         print("ğŸ“ ê²°ê³¼:", output)

import base64
import requests

with open("/root/2025-siseon-eum/ai/data/img/img_001.jpg", "rb") as f:
    image_bytes = f.read()
    image_b64 = base64.b64encode(image_bytes).decode("utf-8")

response = requests.post(
    "http://localhost:8000/vlm/invoke",  # ë˜ëŠ” https://siseon-eum.site/vlm/invoke
    json={
        "input": image_b64
    }
)
print("Status:", response.status_code)
print("Raw response:", response.text)  # ğŸ‘ˆ JSONDecodeError ë‚  ë•ŒëŠ” ì—¬ê¸°ì— ì—ëŸ¬ ë©”ì‹œì§€ ë‚˜ì˜¬ ìˆ˜ë„ ìˆìŒ
try:
    print("Parsed JSON:", response.json())
except Exception as e:
    print("âŒ JSON íŒŒì‹± ì‹¤íŒ¨:", str(e))
